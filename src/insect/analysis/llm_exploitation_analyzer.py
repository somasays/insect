"""Analyzer for detecting LLM and MCP exploitation attempts in repositories.

This module provides detection capabilities for:
- Prompt injection patterns in code/comments/documentation
- MCP (Model Context Protocol) exploitation attempts
- Jailbreak patterns and instructions
- LLM API token extraction attempts
- Context manipulation attacks
- Hidden instruction injection
- Indirect prompt injection through documentation
"""

import base64
import json
import re
import uuid
from pathlib import Path
from typing import Any, Dict, List

import yaml

from insect.analysis import BaseAnalyzer, register_analyzer
from insect.finding import Finding, FindingType, Location, Severity


@register_analyzer
class LLMExploitationAnalyzer(BaseAnalyzer):
    """Detects LLM and MCP exploitation attempts in repositories."""

    name = "llm_exploitation"
    description = "Detects LLM prompt injection and MCP exploitation attempts"
    supported_extensions = {
        # Documentation files
        ".md",
        ".txt",
        ".rst",
        ".adoc",
        ".org",
        # Config files
        ".json",
        ".yaml",
        ".yml",
        ".toml",
        ".ini",
        ".cfg",
        # Code files
        ".py",
        ".js",
        ".ts",
        ".java",
        ".go",
        ".rs",
        ".c",
        ".cpp",
        ".h",
        ".sh",
        ".bash",
        ".zsh",
        ".fish",
        ".ps1",
        ".bat",
        ".cmd",
        # Web files
        ".html",
        ".htm",
        ".xml",
        ".svg",
        # Other
        ".dockerfile",
        ".env",
        ".gitignore",
    }

    # Direct prompt injection patterns
    PROMPT_INJECTION_PATTERNS = [
        # System prompt overrides
        r"(?i)ignore\s+(?:all\s+)?(?:previous\s+)?(?:system\s+)?(?:instructions?|prompts?|rules?)",
        r"(?i)forget\s+(?:all\s+)?(?:previous\s+)?(?:instructions?|prompts?|context)",
        r"(?i)disregard\s+(?:all\s+)?(?:previous\s+)?(?:instructions?|prompts?|rules?)",
        # Role switching
        r"(?i)(?:you\s+are\s+now|now\s+you\s+are|from\s+now\s+on\s+you\s+are)\s+(?:a\s+)?(?:helpful|malicious|different)",
        r"(?i)act\s+as\s+(?:if\s+you\s+are\s+)?(?:a\s+)?(?:hacker|attacker|malicious|evil)",
        r"(?i)pretend\s+(?:to\s+be\s+)?(?:that\s+you\s+are\s+)?(?:a\s+)?(?:different|malicious|evil)",
        # Direct command injection
        r"(?i)execute\s+(?:the\s+following\s+)?(?:command|code|instruction)",
        r"(?i)run\s+(?:the\s+following\s+)?(?:command|code|script)",
        r"(?i)perform\s+(?:the\s+following\s+)?(?:action|operation|task)",
        # Jailbreak attempts
        r"(?i)dan\s+mode|developer\s+mode|god\s+mode|admin\s+mode",
        r"(?i)bypass\s+(?:all\s+)?(?:safety|security|restrictions?|limitations?)",
        r"(?i)unrestricted\s+(?:mode|access|capabilities?)",
        # Context manipulation
        r"(?i)end\s+(?:of\s+)?(?:system\s+)?(?:message|prompt|instruction)",
        r"(?i)new\s+(?:system\s+)?(?:message|prompt|instruction|context)",
        r"(?i)context\s+(?:switch|change|override|reset)",
        # Token extraction
        r"(?i)(?:show|reveal|display|print)\s+(?:your\s+)?(?:api\s+key|token|secret|credentials?)",
        r"(?i)what\s+is\s+your\s+(?:api\s+key|token|secret|password)",
        # Hidden instructions in formatting
        r"<!--\s*(?:ignore|forget|disregard).*?-->",
        r"/\*\s*(?:ignore|forget|disregard).*?\*/",
        r"#\s*(?:ignore|forget|disregard).*",
        # Base64 encoded instructions (common obfuscation)
        r"(?i)(?:base64|b64)\s*[\:\=]\s*[A-Za-z0-9+/]{20,}={0,2}",
    ]

    # MCP-specific exploitation patterns
    MCP_EXPLOITATION_PATTERNS = [
        # Malicious tool definitions
        r"(?i)tool\s*:\s*['\"](?:exec|eval|system|shell|cmd|run)['\"]",
        r"(?i)['\"]type['\"]:\s*['\"](?:function|tool)['\"].*['\"]exec",
        r"(?i)function.*exec.*subprocess|subprocess.*exec.*function",
        # MCP server hijacking
        r"(?i)mcp[_-]?server.*(?:override|replace|hijack)",
        r"(?i)server[_-]?config.*(?:malicious|backdoor|exploit)",
        # Tool capability escalation
        r"(?i)capabilities?.*(?:file_write|file_delete|network_access|system_access)",
        r"(?i)permissions?.*(?:elevated|admin|root|sudo)",
        # Message manipulation
        r"(?i)message[_-]?handler.*(?:override|replace|intercept)",
        r"(?i)protocol[_-]?handler.*(?:bypass|exploit|manipulate)",
    ]

    # Jailbreak trigger phrases
    JAILBREAK_TRIGGERS = [
        # Classic jailbreaks
        "DAN",
        "Developer Mode",
        "Jailbreak Mode",
        "Evil Mode",
        "Do Anything Now",
        "Uncensored Mode",
        "Unrestricted Mode",
        # Roleplay jailbreaks
        "Evil Assistant",
        "Malicious AI",
        "Harmful AI",
        "Dark AI",
        "Unethical AI",
        "Criminal AI",
        "Hacker AI",
        # Emotional manipulation
        "My grandmother is dying",
        "This is an emergency",
        "Life or death situation",
        "You must help me or",
        "If you don't help",
        # Authority figures
        "I am your creator",
        "I am your developer",
        "I am OpenAI",
        "I am Anthropic",
        "This is a test",
        "This is authorized",
        "This is official",
    ]

    # LLM API endpoints and tokens to watch for
    LLM_API_PATTERNS = [
        # API endpoints
        r"(?i)(?:openai|anthropic|cohere|huggingface)\.com",
        r"(?i)api\.(?:openai|anthropic|cohere)\.(?:com|ai)",
        r"(?i)(?:gpt|claude|llama|gemini)[_-]?api",
        # API keys and tokens
        r"(?i)(?:openai|anthropic|cohere)[_-]?(?:api[_-]?)?key",
        r"(?i)(?:gpt|claude|llama|gemini)[_-]?(?:api[_-]?)?(?:key|token)",
        r"sk-[A-Za-z0-9]{20,}",  # OpenAI API key pattern
        r"claude-[A-Za-z0-9-]{20,}",  # Claude API key pattern
    ]

    # File patterns that commonly contain indirect injections
    DOCUMENTATION_FILES = {
        "readme.md",
        "readme.txt",
        "readme.rst",
        "contributing.md",
        "contributing.txt",
        "install.md",
        "installation.md",
        "setup.md",
        "usage.md",
        "examples.md",
        "tutorial.md",
        "docs.md",
        "documentation.md",
        "changelog.md",
        "changes.md",
        "history.md",
    }

    def __init__(self, config: Dict[str, Any]):
        """Initialize the analyzer with configuration.

        Args:
            config: Configuration dictionary
        """
        super().__init__(config)
        llm_config = config.get("llm_exploitation", {})
        self.sensitivity = llm_config.get("sensitivity", "medium")
        self.check_documentation = llm_config.get("check_documentation", True)
        self.check_hidden_instructions = llm_config.get(
            "check_hidden_instructions", True
        )
        self.findings: List[Finding] = []

    def analyze_file(self, file_path: Path) -> List[Finding]:
        """Analyze a file for LLM exploitation attempts.

        Args:
            file_path: Path to the file

        Returns:
            List of findings
        """
        self.findings = []

        # Skip binary files and very large files
        if not self._should_analyze_file(file_path):
            return self.findings

        try:
            content = file_path.read_text(encoding="utf-8", errors="replace")
        except Exception:
            return self.findings

        # Analyze file content
        self._analyze_prompt_injection(file_path, content)
        self._analyze_mcp_exploitation(file_path, content)
        self._analyze_jailbreak_patterns(file_path, content)
        self._analyze_llm_api_abuse(file_path, content)

        # Special analysis for documentation files
        if self.check_documentation and self._is_documentation_file(file_path):
            self._analyze_indirect_injection(file_path, content)

        # Check for hidden instructions
        if self.check_hidden_instructions:
            self._analyze_hidden_instructions(file_path, content)

        # Analyze structured data files
        if file_path.suffix.lower() in {".json", ".yaml", ".yml"}:
            self._analyze_structured_data(file_path, content)

        return self.findings

    def _should_analyze_file(self, file_path: Path) -> bool:
        """Check if file should be analyzed."""
        # Skip very large files (>5MB)
        try:
            if file_path.stat().st_size > 5 * 1024 * 1024:
                return False
        except Exception:
            return False

        # Check file extension or name
        filename = file_path.name.lower()
        file_ext = file_path.suffix.lower()

        # Special handling for files without extensions
        if filename in ["dockerfile", "makefile", "vagrantfile"]:
            return True

        # Skip if extension not supported
        return file_ext in self.supported_extensions or "*" in self.supported_extensions

    def _is_documentation_file(self, file_path: Path) -> bool:
        """Check if file is likely documentation."""
        filename = file_path.name.lower()
        return filename in self.DOCUMENTATION_FILES or any(
            doc_file in filename for doc_file in ["readme", "docs", "guide", "tutorial"]
        )

    def _analyze_prompt_injection(self, file_path: Path, content: str) -> None:
        """Analyze content for direct prompt injection patterns."""
        lines = content.splitlines()

        for line_num, line in enumerate(lines, 1):
            for pattern in self.PROMPT_INJECTION_PATTERNS:
                matches = list(re.finditer(pattern, line, re.IGNORECASE | re.MULTILINE))
                for match in matches:
                    self.findings.append(
                        Finding(
                            id=str(uuid.uuid4()),
                            analyzer=self.name,
                            type=FindingType.SUSPICIOUS,
                            title="Potential prompt injection detected",
                            description=f"Suspicious pattern: '{match.group()}'",
                            location=Location(
                                path=file_path,
                                line_start=line_num,
                                column_start=match.start() + 1,
                                column_end=match.end() + 1,
                            ),
                            severity=Severity.HIGH,
                            confidence=0.8,
                            metadata={
                                "matched_text": match.group(),
                                "pattern": pattern,
                            },
                        )
                    )

    def _analyze_mcp_exploitation(self, file_path: Path, content: str) -> None:
        """Analyze content for MCP exploitation attempts."""
        for pattern in self.MCP_EXPLOITATION_PATTERNS:
            matches = list(re.finditer(pattern, content, re.IGNORECASE | re.DOTALL))
            for match in matches:
                line_num = content[: match.start()].count("\n") + 1
                self.findings.append(
                    Finding(
                        id=str(uuid.uuid4()),
                        analyzer=self.name,
                        type=FindingType.SUSPICIOUS,
                        title="MCP exploitation attempt detected",
                        description=f"Suspicious MCP pattern: '{match.group()}'",
                        location=Location(path=file_path, line_start=line_num),
                        severity=Severity.HIGH,
                        confidence=0.9,
                        metadata={"matched_text": match.group(), "pattern": pattern},
                    )
                )

    def _analyze_jailbreak_patterns(self, file_path: Path, content: str) -> None:
        """Analyze content for jailbreak trigger phrases."""
        content_lower = content.lower()

        for trigger in self.JAILBREAK_TRIGGERS:
            if trigger.lower() in content_lower:
                # Find all occurrences
                start = 0
                while True:
                    pos = content_lower.find(trigger.lower(), start)
                    if pos == -1:
                        break

                    line_num = content[:pos].count("\n") + 1
                    self.findings.append(
                        Finding(
                            id=str(uuid.uuid4()),
                            analyzer=self.name,
                            type=FindingType.SUSPICIOUS,
                            title="Jailbreak trigger phrase detected",
                            description=f"Found jailbreak trigger: '{trigger}'",
                            location=Location(path=file_path, line_start=line_num),
                            severity=Severity.MEDIUM,
                            confidence=0.7,
                            metadata={"trigger": trigger},
                        )
                    )
                    start = pos + 1

    def _analyze_llm_api_abuse(self, file_path: Path, content: str) -> None:
        """Analyze content for LLM API abuse patterns."""
        for pattern in self.LLM_API_PATTERNS:
            matches = list(re.finditer(pattern, content, re.IGNORECASE))
            for match in matches:
                line_num = content[: match.start()].count("\n") + 1

                # Determine severity based on pattern type
                severity = (
                    Severity.HIGH
                    if "key" in pattern.lower() or "token" in pattern.lower()
                    else Severity.MEDIUM
                )

                self.findings.append(
                    Finding(
                        id=str(uuid.uuid4()),
                        analyzer=self.name,
                        type=FindingType.SECRET,
                        title="LLM API credential/endpoint detected",
                        description=f"Potential LLM API usage: '{match.group()}'",
                        location=Location(path=file_path, line_start=line_num),
                        severity=severity,
                        confidence=0.8,
                        metadata={"matched_text": match.group()},
                    )
                )

    def _analyze_indirect_injection(self, file_path: Path, content: str) -> None:
        """Analyze documentation files for indirect prompt injection."""
        # Look for instructions that could be read by LLMs and used as prompts
        indirect_patterns = [
            r"(?i)when\s+(?:an?\s+)?(?:ai|llm|assistant|model)\s+reads?\s+this",
            r"(?i)if\s+(?:an?\s+)?(?:ai|llm|assistant|model)\s+(?:sees?|reads?)\s+this",
            r"(?i)tell\s+(?:the\s+)?(?:ai|llm|assistant|model)\s+to",
            r"(?i)instruct\s+(?:the\s+)?(?:ai|llm|assistant|model)\s+to",
            r"(?i)(?:ai|llm|assistant|model)\s+should\s+(?:ignore|forget|disregard)",
            r"(?i)hidden\s+(?:instruction|prompt|message)",
            r"(?i)secret\s+(?:instruction|prompt|message)",
        ]

        for pattern in indirect_patterns:
            matches = list(re.finditer(pattern, content, re.IGNORECASE))
            for match in matches:
                line_num = content[: match.start()].count("\n") + 1
                self.findings.append(
                    Finding(
                        id=str(uuid.uuid4()),
                        analyzer=self.name,
                        type=FindingType.SUSPICIOUS,
                        title="Indirect prompt injection in documentation",
                        description=f"Potential indirect injection: '{match.group()}'",
                        location=Location(path=file_path, line_start=line_num),
                        severity=Severity.MEDIUM,
                        confidence=0.6,
                        metadata={"matched_text": match.group()},
                    )
                )

    def _analyze_hidden_instructions(self, file_path: Path, content: str) -> None:
        """Analyze content for hidden instructions in various formats."""
        # Check for Base64 encoded instructions
        b64_pattern = r"[A-Za-z0-9+/]{20,}={0,2}"
        b64_matches = re.findall(b64_pattern, content)

        for b64_text in b64_matches:
            try:
                decoded = base64.b64decode(b64_text).decode("utf-8", errors="ignore")
                # Check if decoded text contains suspicious instructions
                if any(
                    keyword in decoded.lower()
                    for keyword in [
                        "ignore",
                        "forget",
                        "execute",
                        "system",
                        "prompt",
                        "instruction",
                    ]
                ):
                    line_num = content.find(b64_text)
                    line_num = (
                        content[:line_num].count("\n") + 1 if line_num != -1 else 1
                    )

                    self.findings.append(
                        Finding(
                            id=str(uuid.uuid4()),
                            analyzer=self.name,
                            type=FindingType.SUSPICIOUS,
                            title="Hidden instruction in Base64 encoding",
                            description=f"Decoded text: '{decoded[:100]}...'",
                            location=Location(path=file_path, line_start=line_num),
                            severity=Severity.HIGH,
                            confidence=0.9,
                            metadata={"encoded": b64_text, "decoded": decoded},
                        )
                    )
            except Exception:
                continue

    def _analyze_structured_data(self, file_path: Path, content: str) -> None:
        """Analyze JSON/YAML files for embedded prompts and tool definitions."""
        try:
            if file_path.suffix.lower() == ".json":
                data = json.loads(content)
            else:  # YAML
                data = yaml.safe_load(content)

            self._scan_dict_for_exploits(file_path, data, "")

        except Exception:
            # If parsing fails, fall back to text analysis
            pass

    def _scan_dict_for_exploits(self, file_path: Path, data: Any, path: str) -> None:
        """Recursively scan dictionary/list data for exploitation patterns."""
        if isinstance(data, dict):
            for key, value in data.items():
                current_path = f"{path}.{key}" if path else key

                # Check for suspicious keys
                if any(
                    suspicious in key.lower()
                    for suspicious in [
                        "prompt",
                        "instruction",
                        "system",
                        "exec",
                        "eval",
                        "command",
                    ]
                ):
                    if isinstance(value, str) and len(value) > 20:
                        # Check if value contains injection patterns
                        for pattern in self.PROMPT_INJECTION_PATTERNS[
                            :5
                        ]:  # Check first few patterns
                            if re.search(pattern, value, re.IGNORECASE):
                                self.findings.append(
                                    Finding(
                                        id=str(uuid.uuid4()),
                                        analyzer=self.name,
                                        type=FindingType.SUSPICIOUS,
                                        title="Suspicious instruction in structured data",
                                        description=f"Key '{key}' contains potential injection: '{value[:100]}...'",
                                        location=Location(path=file_path),
                                        severity=Severity.MEDIUM,
                                        confidence=0.7,
                                        metadata={
                                            "key": key,
                                            "value": value,
                                            "path": current_path,
                                        },
                                    )
                                )
                                break

                self._scan_dict_for_exploits(file_path, value, current_path)

        elif isinstance(data, list):
            for i, item in enumerate(data):
                self._scan_dict_for_exploits(file_path, item, f"{path}[{i}]")
